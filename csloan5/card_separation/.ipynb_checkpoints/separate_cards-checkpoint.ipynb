{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91c65f-d385-43c3-8bd0-61ea525e7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CODE ABANDONED\n",
    "Couldn't get the clustering method to work accurately enough.\n",
    "\n",
    "This script will seperate PDF files based on the visual similarity of their\n",
    "first pages. It will do this using clustering techniques.\n",
    "\n",
    "Written by Cody Sloan\n",
    "\"\"\"\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.cluster import Birch\n",
    "\n",
    "\n",
    "def load_pdf_filenames(pdf_dir):\n",
    "    \"\"\"Takes a path to a directory with PDFs, then adds all of the paths of\n",
    "    the PDF files in that directory to the filenames list, which is returned.\n",
    "    \n",
    "    Parameters:\n",
    "        pdf_dir - String containing file system location of PDF files.\n",
    "    Returns:\n",
    "        filenames - Updated list of PDF file paths.\n",
    "    \"\"\"\n",
    "    filenames = []\n",
    "    # Scan recursively over all pdf files in a directory\n",
    "    for folder, subfolders, files in os.walk(pdf_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                #Add all the .pdf filenames to a list\n",
    "                filename = os.path.join(folder, file)\n",
    "                filenames.append(filename)\n",
    "                \n",
    "    return filenames\n",
    "    \n",
    "    \n",
    "def convert_pdfs_to_imgs(filenames, path_to_poppler):\n",
    "    \"\"\"Takes a list of PDF file paths, and converts the first page of \n",
    "    each PDF to an image, and adds that image to an image list.\n",
    "    \n",
    "    Parameters:\n",
    "        filenames - List of PDF file paths as strings.\n",
    "        path_to_poppler - String containing the location of your poppler\n",
    "            installation. This is usually the /bin/ directory in your \n",
    "            conda environment.\n",
    "    Returns:\n",
    "        imgs - List of images of the first page of each PDF.\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    # Loop over all pdfs and convert the first page to an image\n",
    "    for filename in filenames:\n",
    "        # converts only the pdf files and adds them to the list\n",
    "        img = convert_from_path(filename, \\\n",
    "                                first_page=1, \\\n",
    "                                last_page=1, \\\n",
    "                                poppler_path=path_to_poppler\n",
    "                               )\n",
    "        imgs.append(img[0])\n",
    "        \n",
    "    return imgs\n",
    "\n",
    "\n",
    "def extract_features(images, model):\n",
    "    \"\"\"Takes a list of pillow images and uses a modified neural network model\n",
    "    to extract features from those images.\n",
    "    \n",
    "    Parameters:\n",
    "        images - List of images that are of type 'PIL.Image.Image'.\n",
    "        model - A modified neural network.\n",
    "    Returns:\n",
    "        features - A list of each image's features.\n",
    "    \"\"\"\n",
    "    feat = []\n",
    "    # loop through each image in the dataset\n",
    "    for img in images:\n",
    "        # Resize the image to be 224x224\n",
    "        img = img.resize((224,224))\n",
    "        # convert from 'PIL.Image.Image' to numpy array\n",
    "        img = np.array(img) \n",
    "        # reshape the data for the model reshape\n",
    "        # (num_of_samples, dim 1, dim 2, channels)\n",
    "        reshaped_img = img.reshape(1,224,224,3) \n",
    "        # prepare image for model\n",
    "        imgx = preprocess_input(reshaped_img)\n",
    "        # get the feature vector\n",
    "        feat.append(model.predict(imgx))\n",
    "    \n",
    "    # get a list of just the features\n",
    "    features = np.array(feat)\n",
    "    # Remove the extra image dimension required for the model prediction\n",
    "    features = features.reshape(-1, model.output.shape[1])\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def birch_train(filenames, features, num_clusters, thresh=0.01):\n",
    "    \"\"\"Trains the BIRCH clustering algorithm on inputted features. Creates\n",
    "    <num_clusters> clusters based on those features.\n",
    "    \n",
    "    Parameters:\n",
    "        filenames - List of filenames that need to be clustered.\n",
    "        features - List of features extracted from images.\n",
    "        num_clusters - Target number of clusters.\n",
    "        thresh - Threshold: the maximum number of data points a sub-cluster in\n",
    "            the leaf node of the clustering features tree can hold.\n",
    "    Returns:\n",
    "        clusters - A dictionary where the keys are cluster numbers and the\n",
    "            values are filenames.\n",
    "        birch - The trained BIRCH clustering model.\n",
    "    \"\"\"\n",
    "    # Train the algorithm and find the training set's clusters\n",
    "    birch = Birch(threshold=thresh, n_clusters=num_clusters).fit(features)\n",
    "    # Create the clusters dictionary with filename values\n",
    "    clusters = {}\n",
    "    for file, label in zip(filenames, birch.labels_):\n",
    "        # If the key isn't in the dict, add it\n",
    "        if label not in clusters.keys():\n",
    "            clusters[label] = []\n",
    "        # Add the file to its cluster\n",
    "        clusters[label].append(file)\n",
    "    return clusters, birch\n",
    "\n",
    "\n",
    "def find_clusters(filenames, features, alg, num_clusters, thresh=0.01):\n",
    "    \"\"\"Uses a trained birch clustering algorithm model to cluster files based\n",
    "    on inputted features. Creates <num_clusters> clusters.\n",
    "    \n",
    "    Parameters:\n",
    "        filenames - List of filenames that need to be clustered.\n",
    "        features - List of features extracted from images.\n",
    "        alg - The trained clustering algorithm model.\n",
    "        num_clusters - Target number of clusters.\n",
    "        thresh - Threshold: the maximum number of data points a sub-cluster in\n",
    "            the leaf node of the clustering features tree can hold.\n",
    "    Returns:\n",
    "        clusters - A dictionary where the keys are cluster numbers and the\n",
    "            values are filenames\n",
    "    \"\"\"\n",
    "    # Find which cluster each image belongs to\n",
    "    labels = alg.fit_predict(features)\n",
    "    # Create the clusters dictionary with filename values\n",
    "    clusters = {}\n",
    "    for file, label in zip(filenames, labels):\n",
    "        # If the key isn't in the dict, add it\n",
    "        if label not in clusters.keys():\n",
    "            clusters[label] = []\n",
    "        # Add the file to its cluster\n",
    "        clusters[label].append(file) \n",
    "    return clusters\n",
    "\n",
    "     \n",
    "def view_cluster(cluster_num, cluster_dict, path_to_poppler, page=1):\n",
    "    \"\"\"Allows viewing of images in a cluster. Only 100 images can be displayed\n",
    "    at a time, but this function allows you to choose which 'page' of 100\n",
    "    images you want to see. This is useful when you want to test the\n",
    "    clustering methods in a notebook file.\n",
    "    \n",
    "    Parameters:\n",
    "        cluster_num - Number of cluster you would like to view.\n",
    "        cluster_dict - A dictionary containing files and the clusters they\n",
    "            belong to.\n",
    "        path_to_poppler - String containing the location of your poppler\n",
    "            installation. This is usually the /bin/ directory in your \n",
    "            conda environment.\n",
    "        page - Number of section of 100 images to be viewed in the cluster.\n",
    "            For Ex: page=2 means you would like to view images 100-200 of that\n",
    "            cluster.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (25,25));\n",
    "    # gets the list of filenames for a cluster\n",
    "    files = cluster_dict[cluster_num]\n",
    "    # only allow up to 30 images to be shown at a time\n",
    "    if len(files) > 100:\n",
    "        if page == 1:\n",
    "            print(f\"Loading first 100 files out of {len(files)}\")\n",
    "            files = files[:100]\n",
    "        elif page <= 0:\n",
    "            print(\"Page must be 1 or greater\")\n",
    "        else:\n",
    "            files = files[(page-1)*100 : page*100]\n",
    "            print(f\"Loading files {(page-1)*100}-{((page-1)*100)+len(files)}\")\n",
    "    # plot each image in the cluster\n",
    "    for index, file in enumerate(files):\n",
    "        plt.subplot(10,10,index+1)\n",
    "        img = convert_from_path(file, \\\n",
    "                                first_page=1, \\\n",
    "                                last_page=1, \\\n",
    "                                poppler_path=path_to_poppler\n",
    "                               )\n",
    "        img = np.array(img[0])\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "                \n",
    "        \n",
    "def save_cluster(clusters, save_dir):\n",
    "    \"\"\"Saves the cluster. This will copy the PDF files to their cluster\n",
    "    location in the file system.\n",
    "    \n",
    "    Parameters:\n",
    "        clusters - A dictionary containing files and the clusters they\n",
    "            belong to.\n",
    "        save_dir - Location of cluster subdirectories that files will be\n",
    "            separated into.\n",
    "    \"\"\"\n",
    "    for cluster in clusters:\n",
    "            for file in clusters[cluster]:\n",
    "                full = os.path.join(save_dir, 'cluster'+str(cluster+1), \\\n",
    "                                    os.path.basename(file))\n",
    "                shutil.copyfile(file, full)            \n",
    "    return\n",
    "\n",
    "\n",
    "def separate_dataset(filenames, save_dir, model, path_to_poppler, \\\n",
    "                     algorithm, batch_size=1000):\n",
    "    \"\"\"Seperates the non-training data into their clusters.\n",
    "    \n",
    "    Parameters:\n",
    "        filenames - The names of the files that will be separated.\n",
    "        save_dir - The directory that each cluster directory will be found in.\n",
    "        model - The model that will extract the data's features\n",
    "        path_to_poppler - String containing the location of your poppler\n",
    "            installation. This is usually the /bin/ directory in your \n",
    "            conda environment.\n",
    "        algorithm - The trained clustering algorithm model that will do the\n",
    "            seperating of the files.\n",
    "        batch_size - The size of the batch of files that will be clustered at\n",
    "            a time.\n",
    "    \"\"\"\n",
    "    # Loop over all batches of data\n",
    "    for i in range(0, len(filenames), batch_size):\n",
    "        batch = filenames[i:i+batch_size]\n",
    "        # Grab image data\n",
    "        images = convert_pdfs_to_imgs(batch, path_to_poppler)\n",
    "        # Extract features from images\n",
    "        features = extract_features(images, model)\n",
    "        # Use extracted features to cluster the data\n",
    "        clusters = find_clusters(batch, features, algorithm, num_clusters=4)\n",
    "        # Save the clusters\n",
    "        save_cluster(clusters, save_dir)\n",
    "    return\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c7df0-89b7-42d7-a7e7-5ab65ed50111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set the seed for randomizing data\n",
    "    np.random.seed(123)\n",
    "    \n",
    "    # Load the filenames list and shuffle it randomly\n",
    "    print(\"Loading Filenames ... \", end='')\n",
    "    pdf_dir = r'/project/arcc-students/enhanced_oil_recovery_cards/'\n",
    "    filenames = load_pdf_filenames(pdf_dir)\n",
    "    np.random.shuffle(filenames)\n",
    "    print(\"Finished\")\n",
    "    \n",
    "    # Load the model, removing the fully connected layers so it can be used\n",
    "    # for feature extraction.\n",
    "    print(\"Loading ResNet50 Model ... \", end='')\n",
    "    model = ResNet50()\n",
    "    model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
    "    print(\"Finished\")\n",
    "    \n",
    "    print(\"Training Clustering Algorithm ... \", end='')\n",
    "    # Define the training set\n",
    "    split = 500\n",
    "    train = filenames[:split]\n",
    "    # Convert PDFs in training set to images\n",
    "    path_to_poppler = r'/project/arcc-students/csloan5/environments/GPU_env/bin/'\n",
    "    images = convert_pdfs_to_imgs(train, path_to_poppler)\n",
    "    # Extract features from training data\n",
    "    features = extract_features(images, model)\n",
    "    # Generate clusters from training data, and also train the clustering\n",
    "    # algorithm\n",
    "    clusters, birch = birch_train(train, features, num_clusters=4)\n",
    "    # Save the training data clusters\n",
    "    save_dir = r'/project/arcc-students/csloan5/OilWellCards_project/sorted_cards/'\n",
    "    save_cluster(clusters, save_dir)\n",
    "    print(\"Finished\")\n",
    "    \n",
    "    print(\"Clustering Files ... \", end='')\n",
    "    # Cluster and separate the non-training data\n",
    "    filenames = filenames[split:]\n",
    "    separate_dataset(filenames, save_dir, model, path_to_poppler, birch, \\\n",
    "                     batch_size=500)\n",
    "    print(\"Finished\")\n",
    "    print(\"Separation Completed\")\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc538f13-0a32-4879-a06a-0c12a25e33f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = r'/project/arcc-students/enhanced_oil_recovery_cards/'\n",
    "filenames = load_pdf_filenames(pdf_dir)\n",
    "model = ResNet50()\n",
    "model = Model(inputs = model.inputs, outputs = model.layers[-2].output)\n",
    "train = filenames[:500]\n",
    "# Convert PDFs in training set to images\n",
    "path_to_poppler = r'/project/arcc-students/csloan5/environments/GPU_env/bin/'\n",
    "images = convert_pdfs_to_imgs(train, path_to_poppler)\n",
    "# Extract features from training data\n",
    "features = extract_features(images, model)\n",
    "# Generate clusters from training data, and also train the clustering\n",
    "# algorithm\n",
    "clusters, birch = birch_train(train, features, num_clusters=4)\n",
    "\n",
    "print(\"Total number of Clusters: \", len(clusters))\n",
    "\n",
    "for cluster in clusters:\n",
    "    print(\"Images in cluster\", cluster, len(clusters[cluster]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfb4ab-c41b-4be9-8210-018052535f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPUs",
   "language": "python",
   "name": "gpus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
